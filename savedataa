import json
import os

def run_all_sqls_and_write_files(
    json_path: str,
    base_output_path: str,
    format: str = "parquet"
) -> None:
    """
    Executes all SQL queries defined in a JSON file and saves each result
    to a single file named <ApiName>.<format> in the specified ADLS path.

    Parameters:
        json_path (str): Path to the JSON file on DBFS (e.g. "/mnt/sql/query_list.json").
        base_output_path (str): Output folder on ADLS (e.g. "abfss://.../exports/").
        format (str): Output file format ("parquet", "csv", etc.). Default is "parquet".

    Returns:
        None
    """
    try:
        with open(f"/dbfs{json_path}", "r") as file:
            data = json.load(file)
            query_list = data.get("API", [])

        if not query_list:
            raise ValueError("No 'API' entries found in the JSON.")

        for item in query_list:
            api_name = item.get("ApiName")
            query = item.get("sourceSQL")

            if not api_name or not query:
                print(f"‚ö†Ô∏è Skipping entry due to missing ApiName or sourceSQL: {item}")
                continue

            print(f"üìå Running query for ApiName '{api_name}'...")

            try:
                df = spark.sql(query)

                # Create full file path
                output_file_path = os.path.join(base_output_path, f"{api_name}.{format}")

                # Use coalesce(1) to ensure single file output
                df.coalesce(1).write.mode("overwrite").format(format).save(output_file_path)

                print(f"‚úÖ Saved result to file: {output_file_path}")

            except Exception as qe:
                print(f"‚ùå Error processing '{api_name}': {qe}")

    except Exception as e:
        print(f"‚ùå General error: {e}")
